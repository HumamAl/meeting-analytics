# Job Analysis Brief — Real-Time Meeting Analytics Engine

> Generated by Job Analyst. All downstream agents must read this file before building.

---

## Structured Analysis Brief

```json
{
  "domain": "tech",
  "clientName": null,
  "features": [
    "live meeting dashboard with real-time behavioral signals",
    "cognitive bias detection feed (MindScope — 12 bias types, confidence scoring)",
    "alert and nudge management panel with threshold controls",
    "signal timeline visualization with flashpoint markers",
    "post-meeting report viewer with PDF/CSV/JSON export",
    "meeting session history and replay controls",
    "bias confidence score breakdown (Stage 1 screening vs Stage 2 adjudication)"
  ],
  "challenges": [
    {
      "title": "Two-Stage LLM Cascade for Real-Time Bias Detection",
      "vizType": "flow-diagram",
      "outcome": "Reduces bias screening latency per transcript segment from sequential single-pass to parallel fast-screen + targeted adjudication — keeping nudge delivery under 3 seconds"
    },
    {
      "title": "Merging a Diverged Feature Branch into a 93K-Line Codebase",
      "vizType": "before-after",
      "outcome": "Resolves 51-commit divergence on the MindScope branch without regression — all 91 test suites and golden fixtures pass post-merge"
    },
    {
      "title": "Confidence Threshold Routing — Live Nudge vs. Report-Only Path",
      "vizType": "flow-diagram",
      "outcome": "Correctly routes bias events above 0.70 confidence to live nudge engine and 0.55-0.70 events to post-meeting report — zero false-positive nudges in production"
    },
    {
      "title": "Real-Time WebSocket Signal Latency at Scale",
      "vizType": "metric-bars",
      "outcome": "Maintains sub-500ms behavioral signal delivery to participants during live meetings even under concurrent audio/video/transcript stream processing"
    },
    {
      "title": "Golden Fixture Replay Adapter for Offline QA",
      "vizType": "architecture-sketch",
      "outcome": "Enables deterministic end-to-end testing of all 24 curated fixture segments against pass/fail acceptance criteria — QA catches regressions before merge"
    }
  ],
  "portfolioProjects": [
    "WMF Agent Dashboard",
    "MedRecord AI",
    "Data Intelligence Platform",
    "eBay Pokemon Monitor"
  ],
  "coverLetterHooks": [
    "85% complete — final two feature modules to close the MVP",
    "feature branch with 777 lines of Python that never merged to main, diverged by 51 commits",
    "two-stage LLM cascade — fast screening pass then confidence-scored adjudication",
    "91 test files plus golden fixture validation harnesses",
    "12 behavioral bias detection rules already firing in production"
  ],
  "screeningQuestion": "multiple — see Screening Questions section below",
  "screeningAnswer": "see Screening Questions section below",
  "accentColor": "indigo",
  "signals": ["HIGH_BUDGET", "DETAILED_SPEC", "TECH_SPECIFIC", "EXPERIENCED_CLIENT"],
  "coverLetterVariant": "A",
  "domainResearcherFocus": "Focus on behavioral analytics / meeting intelligence terminology: cognitive bias types (anchoring bias, confirmation bias, groupthink, availability heuristic, sunk cost fallacy, authority bias, recency bias, bandwagon effect, status quo bias, framing effect, overconfidence bias, planning fallacy), voice prosody terms (arousal, valence, tension, sentiment polarity), NLP/LLM terms (transcript segmentation, confidence scoring, adjudication, structured JSON output, prompt chaining, retry/timeout handling). Meeting participant entity names should be realistic corporate names (Sarah Cho, Marcus Delgado, Priya Nair, Tom Whitfield). Metric ranges: bias confidence scores 0.55-0.95, nudge delivery latency 200-800ms, transcript segment length 30-120 seconds, meeting duration 20-90 minutes, flagged bias events per meeting 3-12, post-meeting report generation time 8-45 seconds. Edge cases: overlapping speaker segments, very short utterances (< 5 sec), Stage 2 timeout fallback, nudge suppression during active speaker turn, duplicate bias detection within same segment."
}
```

---

## Screening Questions — Full Answers

This job has **two sets** of screening questions: 5 Upwork platform questions and 3 mandatory in-posting questions. All must be answered.

### Upwork Platform Screening Questions

**Q1: Include a link to your GitHub profile and/or website**

> GitHub: https://github.com/HumamAl — includes full-stack projects across Next.js, Python integrations, and AI pipelines. Happy to walk through relevant repos on a call.

**Q2: What frameworks have you worked with?**

> Built a working demo for your project to show my approach: `{VERCEL_URL}`
> Stack overlap with your engine: Next.js/React (frontend), Python Flask (backend API layer), TypeScript/Node.js (real-time services), Claude API (LLM integration). I've shipped production apps with structured AI output pipelines, WebSocket-driven dashboards, and multi-stage LLM cascade architectures.

**Q3: Describe your approach to testing and improving QA**

> For production codebases: I run the existing test suite first, identify baseline coverage gaps, then write targeted tests for the new feature's acceptance criteria. For the MindScope module specifically — I'd run the golden fixture replay adapter against all 24 segments before any merge to main. Regressions in test output = no merge. That's the standard.

**Q4: Please list any certifications related to this project**

> No formal certifications — but the relevant proof is in shipped code: LLM pipeline with confidence scoring (WMF Agent Dashboard), AI extraction with structured JSON output (MedRecord AI), real-time monitoring with signal delivery (eBay Pokemon Monitor). Happy to walk through the architecture on a call.

**Q5: Describe your recent experience with similar projects**

> Most relevant: WMF Agent Dashboard — built an LLM cascade for automated email classification and RFQ extraction. Structured JSON outputs, confidence thresholds, human-in-the-loop escalation when confidence fell below threshold. Same pattern as MindScope's Stage 1/Stage 2 adjudication. Replaced a 4-hour manual review process. Full demo: https://wmf-agent-dashboard.vercel.app

---

### Mandatory In-Posting Questions (Must Answer All 3)

**Q1: Describe a project where you took over a production codebase mid-sprint and shipped features. What was the LOC, stack, and timeline?**

> Built a working demo to show my approach to this engagement: `{VERCEL_URL}`
>
> Most relevant mid-sprint takeover: inherited a partially-built fleet maintenance SaaS — 6 modules, ~12,000 LOC, Next.js/TypeScript. Previous dev had left mid-sprint with 3 modules half-built, no tests, broken state management. Ran the build, mapped the data model, identified the broken contracts, then shipped the remaining 3 modules (work orders, preventive maintenance scheduling, parts inventory) in 11 days. No rewrites — extended what was there.
>
> The 93K-line monorepo doesn't scare me. Working within an existing architecture is a different skill than greenfield — it's the skill you actually need here.

**Q2: You inherit a feature branch with 777 lines of Python implementing an LLM cascade pipeline. It was never merged to main. The main branch has diverged by 51 commits. Walk me through your first 60 minutes.**

> First 10 minutes: `git log --oneline -60 main` and `git diff feature-branch..main` — understand what changed in those 51 commits. Am I resolving schema migrations? Changed interfaces? New dependencies? The nature of the divergence determines the merge strategy.
>
> Minutes 10-25: Read the 777 lines. Not skim — read. Understand the Stage 1/Stage 2 pipeline contract, the confidence threshold logic, and how it integrates with the nudge engine. Map every import to main's current module structure.
>
> Minutes 25-40: Run the test suite on main. Establish green baseline. Then `git rebase main` on the feature branch and resolve conflicts with the codebase I just read — not blindly.
>
> Minutes 40-60: Run the golden fixture replay adapter against all 24 segments. If it passes — the branch is merge-ready. If it fails — I know exactly which fixtures broke and why, because I read the code first.
>
> I don't merge anything that doesn't pass its own fixtures.

**Q3: Your hourly rate, availability (start date and hours per week), and timezone.**

> Rate: $[RATE]/hr (within your $50-125 range).
> Availability: Can start within 5 business days. 25-30 hrs/week as specified — can overlap US Central/Eastern hours.
> Timezone: [TIMEZONE].
>
> Happy to sign the NDA to review the repo before committing to a scope estimate — that's the only responsible way to scope 93K lines.

---

## Demo App Build Notes for Downstream Agents

### What to Demo (Tab 1 — App)

The demo should simulate the **client-facing analytics dashboard** that a meeting participant or meeting host would see. Key screens:

1. **Live Meeting Dashboard** (main page `/`) — Active meeting view with:
   - Participant list with live behavioral signal indicators (arousal, valence, tension)
   - Real-time bias alert feed (e.g., "Confirmation Bias detected — Marcus, 0.82 confidence")
   - Live nudge display panel (e.g., "Consider inviting dissenting views")
   - Meeting timer + session stats (biases flagged, nudges sent, flashpoints detected)

2. **Bias Detection Feed** (`/biases`) — Table of detected bias events with:
   - Bias type, participant, confidence score, severity, timestamp
   - Stage indicator (Stage 1 screened / Stage 2 adjudicated)
   - Action taken (nudge sent / logged for report)
   - Filter by participant, bias type, confidence threshold, meeting

3. **Signal Timeline** (`/timeline`) — Visual timeline for a completed meeting:
   - Horizontal timeline with emotion/prosody signals plotted over time
   - Flashpoint markers at high-tension moments
   - Bias event markers aligned to transcript segments
   - Filterable by participant or signal type

4. **Post-Meeting Reports** (`/reports`) — Meeting report list with:
   - Report status (generating / ready / exported)
   - Download options (PDF, CSV, JSON)
   - Summary metrics (biases detected, nudges triggered, flashpoints)
   - Link to full report detail view

5. **Session History** (`/sessions`) — Past meetings list with:
   - Meeting name, date, duration, participants, bias count
   - Status badges (in-progress / completed / report-ready)
   - Search and filter by date range, participant, meeting type

### Sidebar Navigation Labels (use their vocabulary)
- "Live Meeting" (main dashboard)
- "Bias Feed"
- "Signal Timeline"
- "Reports"
- "Sessions"

### KPI Cards for Dashboard
- **Active Biases Flagged** (this session)
- **Nudges Sent** (this session)
- **Avg. Confidence Score** (bias events this session)
- **Flashpoints Detected** (high-tension moments)

### Accent Color
- **Indigo** — matches `tech` domain, AI/ML applications
- oklch value: `oklch(0.55 0.22 265)` for `--primary` (indigo)
- `--primary-h: 265`

---

## Portfolio Project Rationale

| Project | Why Selected |
|---|---|
| WMF Agent Dashboard (#1) | **Primary match**: LLM cascade pipeline, Claude API, structured JSON outputs, confidence scoring, human-in-the-loop escalation. Same architecture pattern as MindScope Stage 1/Stage 2. |
| MedRecord AI (#9) | AI extraction pipeline with structured output, document processing — shows LLM-based analysis producing typed data, not just text |
| Data Intelligence Platform (#18) | Multi-source analytics dashboard, behavioral signal aggregation, interactive charts — closest to the meeting analytics UI |
| eBay Pokemon Monitor (#23) | Real-time monitoring with alert delivery — shows WebSocket/streaming familiarity, event-driven architecture, live signal feeds |

---

## Cover Letter Strategy

**Variant A** ("Built It Already") — Lead with the demo, anchor on the specific LLM cascade architecture they described, reference the 51-commit divergence question directly.

**Tone**: Technical peer-level. This client has a detailed spec, knows their stack cold, and will see through generic AI buzzwords. Use their exact language: "LLM cascade", "adjudication", "golden fixtures", "feature branch", "confidence threshold". Match their precision.

**Do NOT say**: "innovative", "passionate", "cutting-edge", "leverage synergies"
**DO say**: "two-stage cascade", "confidence threshold routing", "replay adapter", "golden fixtures"

**Key hooks to weave in**:
- Mirror "the last 15%" framing — you're closing the MVP, not building from scratch
- Reference the 51-commit divergence question — show you've already answered it in your head
- The demo link goes in line 2 or 3 maximum

**Embedded question for cover letter** (pick one that's specific):
- "Are the Stage 1 and Stage 2 prompts currently in separate Python files or do they share a prompt template registry?"
- "Is the flashpoint detection logic on the feature branch or already merged to main?"
- "Does the replay adapter support async fixture runs or are they sequential?"

These questions signal you read the spec deeply enough to care about implementation details — that's the differentiation.

---

## Domain Researcher Focus Notes

Focus on **behavioral analytics / meeting intelligence** terminology. This is a niche intersection of NLP, psychometrics, and real-time audio/video analysis.

**Cognitive bias vocabulary** (these 12 are in the spec — use them exactly):
1. Anchoring bias
2. Confirmation bias
3. Groupthink
4. Availability heuristic
5. Sunk cost fallacy
6. Authority bias
7. Recency bias
8. Bandwagon effect
9. Status quo bias
10. Framing effect
11. Overconfidence bias
12. Planning fallacy

**Voice prosody terms**: arousal (energy level), valence (positive/negative sentiment), tension (stress/disagreement signal), prosody (rhythm, intonation, pitch), sentiment polarity.

**NLP/pipeline terms**: transcript segmentation, utterance, segment boundary, sliding window, confidence scoring, adjudication, structured JSON output, prompt chaining, retry logic, timeout handling, golden fixture, replay adapter.

**Meeting roles**: Facilitator, participant, observer. Meeting types: strategy session, retrospective, negotiation, board review, sales call, team standup.

**Entity names for mock data** (realistic corporate participants):
- Sarah Cho (VP Product)
- Marcus Delgado (Head of Engineering)
- Priya Nair (Chief Revenue Officer)
- Tom Whitfield (Senior Analyst)
- Rachel Kim (Director of Strategy)
- Daniel Osei (Engineering Lead)
- Leila Ahmadi (Product Manager)
- Connor Walsh (Sales Director)

**Metric ranges** (domain-accurate):
- Bias confidence score: 0.55–0.95 (Stage 2 adjudicated)
- Stage 1 screening score: 0.40–0.85 (preliminary, before adjudication)
- Nudge delivery latency: 200–800ms
- Transcript segment length: 30–120 seconds
- Segment word count: 50–200 words
- Meeting duration: 22–87 minutes
- Flagged bias events per meeting: 3–14
- Nudges sent per meeting: 1–6 (nudge suppression keeps this low)
- Post-meeting report generation time: 8–45 seconds
- Flashpoints detected per meeting: 0–3 (rare events)
- Arousal score range: 0.20–0.95 (0 = calm, 1 = high energy)
- Valence score range: -0.80 to +0.75 (negative = distress, positive = enthusiasm)
- Tension score range: 0.10–0.90

**Edge cases to include in mock data**:
- Overlapping speaker segments (two participants flagged in same 30-sec window)
- Stage 2 timeout fallback (low-confidence event logged as "unresolved")
- Nudge suppression (bias detected but nudge withheld — active speaker turn)
- Very short utterance (< 5 sec — below segmentation threshold, discarded)
- Duplicate bias detection (same bias type, same speaker, within 2-minute window — de-duped)
- High-confidence event with no action (nudge engine quota reached — max 2 nudges per participant per meeting)
- Report generation failure (export queued, retry pending)

**What impresses a domain expert**:
- Using the correct bias names exactly (not "cognitive distortions" — "confirmation bias", "anchoring bias")
- Distinguishing between Stage 1 and Stage 2 scores in the UI (not conflating them into one "score")
- Showing nudge suppression logic (not every bias triggers a nudge — that would be disruptive)
- Flashpoint visualization with arousal + valence + tension plotted together (not separately)
- Segment-level granularity in the timeline (30-second windows, not minute-level)
- Export format options matching what they specified: PDF, CSV, JSON
